{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import random\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rete Semplificata\n",
    "###### 3 layers, 6 neuroni, 9 pesi e 4 biases\n",
    "<img src=\"./img/rete_semplificata_3.svg\" width=\"300px\" />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "w\n",
      "[[ 0.12240879 -1.15445244]\n",
      " [ 0.64723907 -1.92025825]\n",
      " [-0.02849609  1.04300091]]\n",
      "\n",
      "a\n",
      "[1, 2]\n",
      "\n",
      "np.dot(w, a)\n",
      "[-2.18649609 -3.19327742  2.05750574]\n",
      "\n",
      "b\n",
      "[[-0.123468  ]\n",
      " [ 1.40118644]\n",
      " [-0.45109353]]\n",
      "\n",
      "np.dot(w, a)+b\n",
      "[[-2.3099641  -3.31674542  1.93403773]\n",
      " [-0.78530965 -1.79209097  3.45869218]\n",
      " [-2.63758963 -3.64437095  1.6064122 ]]\n",
      "\n",
      "sigmoid(np.dot(w, a)+b)\n",
      "[[0.09030109 0.03500117 0.87369566]\n",
      " [0.31317666 0.14281656 0.96948931]\n",
      " [0.06675805 0.02547206 0.83291267]]\n",
      "\n",
      "w\n",
      "[[-0.14258062 -1.47871857 -0.51700579]]\n",
      "\n",
      "a\n",
      "[[0.09030109 0.03500117 0.87369566]\n",
      " [0.31317666 0.14281656 0.96948931]\n",
      " [0.06675805 0.02547206 0.83291267]]\n",
      "\n",
      "np.dot(w, a)\n",
      "[[-0.51048963 -0.22934518 -1.98879458]]\n",
      "\n",
      "b\n",
      "[[0.10083143]]\n",
      "\n",
      "np.dot(w, a)+b\n",
      "[[-0.4096582  -0.12851375 -1.88796315]]\n",
      "\n",
      "sigmoid(np.dot(w, a)+b)\n",
      "[[0.39899408 0.46791571 0.13147688]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(array([[ 0.12240879, -1.15445244],\n",
       "         [ 0.64723907, -1.92025825],\n",
       "         [-0.02849609,  1.04300091]]),\n",
       "  array([[-0.123468  ],\n",
       "         [ 1.40118644],\n",
       "         [-0.45109353]])),\n",
       " (array([[-0.14258062, -1.47871857, -0.51700579]]), array([[0.10083143]]))]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class Network():\n",
    "\n",
    "    def __init__(self, sizes):\n",
    "        self.num_layers = len(sizes)\n",
    "        \n",
    "        self.sizes = sizes\n",
    "\n",
    "        # self.biases = [np.random.randn(y, 1) for y in sizes[1:]]\n",
    "        self.biases = []\n",
    "        for y in sizes[1:]:\n",
    "            self.biases.append(np.random.randn(y, 1))\n",
    "\n",
    "        # per ogni layer (escluso il primo [:1])\n",
    "        # appendo una matrice larga 1 e alta y\n",
    "        # np.random.randn(y, 1) genera una matrice larga y e lunga x piena di numeri casuale generati secondo la ditribuzione gaussiana\n",
    "\n",
    "        [\n",
    "            [ # biases del 2° layer\n",
    "                [-0.18229955], # bias del neurone c\n",
    "                [ 0.55954105], # bias del neurone d\n",
    "                [-0.1041088 ]  # bias del neurone e\n",
    "            ],\n",
    "            [ # bias del 3° layer\n",
    "                [-0.29721879] # bias del neurone f\n",
    "            ]\n",
    "        ]\n",
    "\n",
    "\n",
    "\n",
    "        # self.weights = [np.random.randn(y, x) for x, y in zip(sizes[:-1], sizes[1:])]\n",
    "        self.weights = []\n",
    "        for x, y in zip(sizes[:-1], sizes[1:]): # sizes[:-1] numero di layer indice i, sizes[:-1] numero di layer indice i + 1\n",
    "            self.weights.append(np.random.randn(y, x))\n",
    "\n",
    "        # print(list(zip(sizes[:-1], sizes[1:])))\n",
    "\n",
    "        [\n",
    "            [ # secondo layer di neuroni\n",
    "                [-0.0696123 , -1.18933377], # pesi collegati al 1° dei 3 neuroni presenti nel 2° layer (rossi)\n",
    "                [-0.04974523,  0.9851919 ], # pesi collegati al 2° dei 3 neuroni presenti nel 2° layer (verdi)\n",
    "                [ 1.34270952,  0.93067125]  # pesi collegati al 3° dei 3 neuroni presenti nel 2° layer (blu)\n",
    "            ], \n",
    "            [ # terzo layer di neuroni\n",
    "                [-0.22923997,  1.76679014,  1.11225243] # pesi collegati all'unico neurone presente nel 3° layer (gialli)\n",
    "            ]\n",
    "        ]\n",
    "\n",
    "    def feedforward(self, a):\n",
    "        \"\"\" return the output of the network if \"a\" is input \"\"\"\n",
    "\n",
    "        # print(\"self.biases\")\n",
    "        # print(self.biases)\n",
    "        # print(\"\\nself.weights\")\n",
    "        # print(self.weights)\n",
    "\n",
    "        # print(\"\\nlist(zip(self.biases, self.weights))\")\n",
    "        # print(list(zip(self.biases, self.weights)))\n",
    "\n",
    "        [\n",
    "            ( # layer 2\n",
    "                [ # biases dei 3 neuroni del layer 2\n",
    "                    [ 0.20510307],\n",
    "                    [-0.91035791],\n",
    "                    [-0.02419269]\n",
    "                ],\n",
    "        \n",
    "                [ # weights dei 3 neuroni del layer 2\n",
    "                    [ 1.62232805,  1.19166118], # rossi\n",
    "                    [-2.64924593,  0.36606946], # verdi\n",
    "                    [ 1.33485666,  0.47252189]  # blu\n",
    "                ]\n",
    "            ), \n",
    "       \n",
    "            ( #layer 3\n",
    "                [ # bias del neurone del layer 3\n",
    "                    [0.11012774]\n",
    "                ],\n",
    "             \n",
    "                [ # weights del neurone del layer 3\n",
    "                    [-0.4695252 , -1.41941377,  0.77848783] # gialli\n",
    "                ]\n",
    "            )\n",
    "        ]\n",
    "\n",
    "        for b, w in zip(self.biases, self.weights): # per ogni layer\n",
    "\n",
    "            # esempio del primo layer\n",
    "            print(\"\\nw\")\n",
    "            print(w)\n",
    "\n",
    "            # weight\n",
    "            # [\n",
    "            #   [ a  b ] \n",
    "            #   [ c  d ]\n",
    "            #   [ e  f ]\n",
    "            #  ]\n",
    "\n",
    "            print(\"\\na\")\n",
    "            print(a)\n",
    "\n",
    "            # input\n",
    "            # [x, y]\n",
    "\n",
    "            print(\"\\nnp.dot(w, a)\")\n",
    "            print(np.dot(w, a))\n",
    "\n",
    "            # np.dot(w, a)\n",
    "            # [ a*x + b*y, c*x + d*y, e*x + f*y ]\n",
    "\n",
    "            print(\"\\nb\")\n",
    "            print(b)\n",
    "\n",
    "            # biases\n",
    "            # [ b1, b2, b3 ]\n",
    "\n",
    "            print(\"\\nnp.dot(w, a)+b\")\n",
    "            print(np.dot(w, a)+b)\n",
    "\n",
    "            # np.dot(w, a) + b\n",
    "            # [ a*x + b*y + b1,\n",
    "            #   c*x + d*y + b2,\n",
    "            #   e*x + f*y + b3 ]\n",
    "\n",
    "            print(\"\\nsigmoid(np.dot(w, a)+b)\")\n",
    "            print(self.sigmoid(np.dot(w, a)+b))\n",
    "            \n",
    "            a = self.sigmoid(np.dot(w, a)+b)\n",
    "\n",
    "\n",
    "        return a\n",
    "\n",
    "    def SGD(self, training_data, epochs, mini_batch_size, eta, test_data=None):\n",
    "        \"\"\" SGD: mini-batch Stochastic (random) Gradient Descent \"\"\"\n",
    "\n",
    "        if test_data: \n",
    "            n_test = len(test_data)\n",
    "\n",
    "        n = len(training_data)\n",
    "\n",
    "        for j in range(epochs):\n",
    "            random.shuffle(training_data)\n",
    "\n",
    "            mini_batches = [\n",
    "                training_data[k:k+mini_batch_size]\n",
    "                for k in range(0, n, mini_batch_size)]\n",
    "            \n",
    "            for mini_batch in mini_batches:\n",
    "                self.update_mini_batch(mini_batch, eta)\n",
    "\n",
    "            if test_data:\n",
    "                print(f\"Epoch {j}: {self.evaluate(test_data)} / {n_test}\")\n",
    "            else:\n",
    "                print(f\"Epoch {j} complete\")\n",
    "\n",
    "    \n",
    "    def sigmoid(self, z):\n",
    "        \"\"\" Sigmoid function \"\"\"\n",
    "        return 1.0/(1.0+np.exp(-z))\n",
    "    \n",
    "    def sigmoid_prime(self, z):\n",
    "        \"\"\" Derivative of the sigmoid function \"\"\"\n",
    "        return self.sigmoid(z)*(1-self.sigmoid(z))\n",
    "\n",
    "    def export(self): # da completare\n",
    "        return list(zip(self.weights, self.biases))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "net = Network([2, 3, 1])\n",
    "\n",
    "net.feedforward([1, 2])\n",
    "\n",
    "net.export()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
